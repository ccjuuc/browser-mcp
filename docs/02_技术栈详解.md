# 技术栈详解

本文档详细介绍 Browser MCP Server 项目使用的核心技术栈，包括每个技术的选择理由、基本概念和使用方式。

## 1. Rust 编程语言

### 1.1 为什么选择 Rust？

Rust 是本项目的核心编程语言，选择它的原因：

- **内存安全**：编译期保证内存安全，避免常见的内存错误
- **性能**：零成本抽象，性能接近 C/C++
- **并发安全**：所有权系统保证并发安全，避免数据竞争
- **丰富的生态系统**：拥有优秀的异步运行时和库

### 1.2 关键 Rust 特性使用

#### 所有权系统（Ownership）

```rust
// 所有权转移示例
pub fn chunk_file(&self, file_path: &str) -> Result<Vec<CodeChunk>> {
    // self 是借用的引用，不转移所有权
    // 返回 Vec 时所有权转移给调用者
}
```

#### 异步编程

```rust
// 使用 async/await
pub async fn search_code(&self, query: &str, max_results: usize) -> Result<Vec<SearchResult>> {
    // 异步函数，使用 await 等待异步操作完成
}
```

#### 错误处理

```rust
// 使用 Result 类型进行错误处理
use anyhow::Result;

pub async fn read_file(&self, file_path: &str) -> Result<String> {
    fs::read_to_string(&full_path)
        .await
        .with_context(|| format!("Failed to read file: {}", file_path))
}
```

### 1.3 项目使用的 Rust 版本

- **Edition**: 2021
- **最低支持版本**: Rust 1.70+

## 2. Tokio 异步运行时

### 2.1 什么是 Tokio？

Tokio 是 Rust 最流行的异步运行时，提供：

- **异步 I/O**：非阻塞的网络和文件 I/O
- **任务调度**：高效的异步任务调度器
- **计时器**：异步定时器支持
- **同步原语**：异步版本的锁、通道等

### 2.2 在项目中的使用

#### 异步主函数

```rust
#[tokio::main]
async fn main() -> Result<()> {
    // 启动异步运行时
    // 所有异步操作在这里执行
}
```

#### 异步文件操作

```rust
use tokio::fs;

// 异步读取文件，不阻塞线程
let content = fs::read_to_string(&file_path).await?;
```

#### 并发任务

```rust
// 启动后台任务
tokio::spawn(async move {
    // 后台索引任务
    indexer.index_codebase().await
});
```

#### 阻塞任务处理

```rust
// Tree-sitter 是同步的，需要在阻塞线程中运行
let chunks = tokio::task::spawn_blocking(move || {
    parser.parse_file(&path, &content, chunk_size)
}).await?;
```

### 2.3 Tokio 特性

项目启用了 Tokio 的 `full` 特性，包含：
- `fs`: 异步文件系统
- `io-std`: 标准输入输出
- `io-util`: I/O 工具
- `net`: 网络支持
- `rt-multi-thread`: 多线程运行时
- `time`: 时间操作

## 3. Tree-sitter 代码解析

### 3.1 什么是 Tree-sitter？

Tree-sitter 是一个增量解析器生成工具和运行时库：

- **增量解析**：只重新解析变更的部分
- **容错性**：即使代码有语法错误也能解析
- **多语言支持**：支持 40+ 种编程语言
- **快速**：解析速度非常快

### 3.2 工作原理

1. **语法定义**：每种语言有对应的语法定义文件
2. **解析器生成**：根据语法生成解析器
3. **生成语法树**：解析代码生成抽象语法树（AST）
4. **查询**：可以查询特定类型的节点

### 3.3 在项目中的使用

#### 支持的语言

```rust
pub enum LanguageType {
    Rust,          // tree-sitter-rust
    JavaScript,    // tree-sitter-javascript
    TypeScript,    // tree-sitter-typescript
    Python,        // tree-sitter-python
    Cpp,           // tree-sitter-cpp
    C,             // tree-sitter-c
    Java,          // tree-sitter-java
    Go,            // tree-sitter-go
    Json,          // tree-sitter-json
    Yaml,          // tree-sitter-yaml
    Html,          // tree-sitter-html
    Css,           // tree-sitter-css
}
```

#### 解析流程

```rust
// 1. 创建解析器
let mut parser = Parser::new();

// 2. 设置语言
parser.set_language(&language)?;

// 3. 解析代码
let tree = parser.parse(content, None)?;

// 4. 遍历语法树
let root_node = tree.root_node();
// 提取函数、类等节点
```

#### 节点提取

```rust
// 提取特定类型的节点（如函数定义）
let target_types = vec!["function_item", "struct_item", "class_item"];

// 遍历树查找匹配的节点
for node in traverse_tree(...) {
    if target_types.contains(&node.kind()) {
        // 提取代码块
    }
}
```

### 3.4 为什么使用 Tree-sitter？

- **语义理解**：可以识别代码结构，而不仅仅是文本
- **准确性**：基于语法规则，比正则表达式更准确
- **统一接口**：不同语言使用相同的 API
- **性能好**：增量解析，适合大型代码库

## 4. Candle 深度学习框架

### 4.1 什么是 Candle？

Candle 是 Rust 编写的极简机器学习框架：

- **纯 Rust**：无需 Python 运行时
- **GPU 支持**：支持 CUDA（NVIDIA GPU）
- **ONNX 兼容**：可以加载 ONNX 模型
- **轻量级**：专为推理设计，占用资源少

### 4.2 在项目中的使用

#### 模型加载

```rust
use candle_core::{Device, Tensor};
use candle_transformers::models::bert::{BertModel, Config as BertConfig};

// 1. 选择设备（GPU 或 CPU）
let device = Device::cuda_if_available(0)?;

// 2. 加载模型配置
let config = BertConfig::from_file("config.json")?;

// 3. 加载模型权重
let vb = VarBuilder::from_mmaped_safetensors(&["model.safetensors"], DType::F32, &device)?;
let model = BertModel::load(vb, &config)?;
```

#### 推理过程

```rust
// 1. 分词
let encoding = tokenizer.encode(text, true)?;

// 2. 创建输入张量
let input_ids = Tensor::new(input_ids, &device)?.unsqueeze(0)?;
let attention_mask = Tensor::new(attention_mask, &device)?.unsqueeze(0)?;

// 3. 前向传播
let hidden_states = model.forward(&input_ids, &token_type_ids, Some(&attention_mask))?;

// 4. 提取 [CLS] token 的嵌入
let cls_embedding = hidden_states.get(0)?.get(0)?;

// 5. 归一化
let embedding = normalize(cls_embedding.to_vec1::<f32>()?);
```

### 4.3 CUDA 支持

```toml
# Cargo.toml
candle-core = { version = "0.8.2", features = ["cuda"], optional = true }
```

- 如果系统有 NVIDIA GPU 和 CUDA，自动使用 GPU
- 如果没有 GPU，自动降级到 CPU
- GPU 推理速度通常比 CPU 快 10-100 倍

### 4.4 模型格式

项目支持两种模型格式：

1. **SafeTensors**（推荐）：
   - 安全、快速的格式
   - 无需额外的依赖
   - 支持内存映射，节省内存

2. **PyTorch (.bin)**：
   - 可以自动转换为 SafeTensors
   - 使用提供的转换脚本

## 5. Qdrant 向量数据库

### 5.1 什么是 Qdrant？

Qdrant 是开源的向量数据库：

- **高性能**：专门优化向量相似度搜索
- **可扩展**：支持集群部署
- **功能丰富**：支持过滤、分片等
- **易用**：提供 REST API 和 gRPC API

### 5.2 在项目中的使用

#### 连接和初始化

```rust
use qdrant_client::Qdrant;

// 创建客户端
let client = Qdrant::from_url("http://localhost:6334")
    .skip_compatibility_check()
    .build()?;

// 创建集合（如果不存在）
if !client.collection_exists("browser-mcp").await? {
    client.create_collection(CreateCollection {
        collection_name: "browser-mcp".to_string(),
        vectors_config: Some(VectorsConfig {
            config: Some(VectorParams {
                size: 768,  // CodeBERT 向量维度
                distance: Distance::Cosine.into(),
            }),
        }),
    }).await?;
}
```

#### 插入向量

```rust
let points = vec![PointStruct::new(
    id.to_string(),      // 点 ID
    embedding,           // 向量
    payload,             // 元数据（代码块信息）
)];

client.upsert_points(UpsertPoints {
    collection_name: "browser-mcp".to_string(),
    points,
}).await?;
```

#### 向量搜索

```rust
let results = client.search_points(SearchPoints {
    collection_name: "browser-mcp".to_string(),
    vector: query_vector,  // 查询向量
    limit: 10,             // 返回数量
    with_payload: Some(true.into()),
}).await?;
```

### 5.3 自动下载和启动

项目会自动：

1. **检测 Qdrant 是否运行**：检查端口 6334
2. **下载二进制**：如果没有找到，自动从 GitHub 下载
3. **启动服务**：在后台启动 Qdrant 进程
4. **配置存储**：设置数据存储路径

### 5.4 存储结构

每个向量点包含：

- **ID**：基于文件路径和位置的唯一标识
- **向量**：768 维浮点向量（CodeBERT 输出）
- **Payload**（元数据）：
  - `file_path`: 文件路径
  - `language`: 编程语言
  - `content`: 代码内容
  - `start_line`, `end_line`: 行号范围
  - `node_type`: 节点类型
  - `node_name`: 节点名称

## 6. Axum Web 框架

### 6.1 什么是 Axum？

Axum 是基于 Tokio 的现代 Web 框架：

- **类型安全**：利用 Rust 的类型系统
- **性能高**：零成本抽象
- **易用**：简洁的 API
- **异步**：完全异步

### 6.2 在项目中的使用

#### 路由设置

```rust
use axum::{Router, routing::{get, post}};

let app = Router::new()
    .route("/", post(handle_mcp_request))  // MCP 请求处理
    .route("/health", get(health_check))   // 健康检查
    .layer(CorsLayer::new()                // CORS 支持
        .allow_origin(Any)
        .allow_methods(Any))
    .with_state(indexer);                  // 共享状态
```

#### 请求处理

```rust
async fn handle_mcp_request(
    State(indexer): State<Arc<CodebaseIndexer>>,
    Json(request): Json<Value>,
) -> impl IntoResponse {
    // 处理 JSON-RPC 请求
    let mcp_server = MCPServer::new(indexer);
    match mcp_server.handle_json_request(request).await {
        Ok(response) => (StatusCode::OK, Json(response)),
        Err(e) => (StatusCode::INTERNAL_SERVER_ERROR, Json(error)),
    }
}
```

### 6.3 CORS 支持

项目启用了 CORS，允许：

- 任何来源（`allow_origin(Any)`）
- 任何 HTTP 方法
- 任何请求头

这允许 Web 客户端直接调用 API。

## 7. Serde 序列化框架

### 7.1 什么是 Serde？

Serde 是 Rust 的序列化/反序列化框架：

- **零成本**：编译期生成代码，运行时性能高
- **格式支持**：支持 JSON、TOML、YAML 等
- **灵活**：支持自定义序列化逻辑

### 7.2 在项目中的使用

#### 结构体序列化

```rust
use serde::{Serialize, Deserialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct CodeChunk {
    pub file_path: String,
    pub content: String,
    // ...
}
```

#### JSON 处理

```rust
use serde_json;

// 序列化
let json = serde_json::to_string(&chunk)?;

// 反序列化
let chunk: CodeChunk = serde_json::from_str(&json)?;
```

#### TOML 配置

```rust
use toml;

// 从 TOML 加载配置
let config: Config = toml::from_str(&content)?;
```

## 8. Tokenizers 库

### 8.1 什么是 Tokenizers？

Tokenizers 是 Hugging Face 的分词器库的 Rust 实现：

- **高性能**：Rust 实现，速度快
- **兼容性**：与 Hugging Face 分词器兼容
- **多格式支持**：支持 BPE、WordPiece 等

### 8.2 在项目中的使用

#### 加载分词器

```rust
use tokenizers::Tokenizer;

// 从文件加载
let tokenizer = Tokenizer::from_file("tokenizer.json")?;
```

#### 分词

```rust
// 编码文本
let encoding = tokenizer.encode(text, true)?;

// 获取 token IDs
let input_ids: Vec<u32> = encoding
    .get_ids()
    .iter()
    .map(|&id| id as u32)
    .collect();
```

### 8.3 分词器格式

项目支持两种格式：

1. **tokenizer.json**（推荐）：
   - Hugging Face 的标准格式
   - 包含完整的分词器信息

2. **vocab.json + merges.txt**（旧格式）：
   - 可以自动构建 tokenizer
   - 兼容旧模型

## 9. 其他重要依赖

### 9.1 anyhow - 错误处理

```rust
use anyhow::{Result, Context};

// 提供便捷的错误处理
let content = fs::read_to_string(path)
    .with_context(|| format!("Failed to read: {}", path))?;
```

### 9.2 regex - 正则表达式

```rust
use regex::Regex;

// 文本搜索中的模式匹配
let regex = Regex::new(&regex::escape(&query))?;
if regex.is_match(&line) {
    // 匹配成功
}
```

### 9.3 ignore - 文件过滤

```rust
use ignore::WalkBuilder;

// 尊重 .gitignore 的文件遍历
let walker = WalkBuilder::new(&path)
    .git_ignore(true)
    .build();
```

### 9.4 tracing - 日志框架

```rust
use tracing::{info, warn, error};

// 结构化日志
tracing::info!("Starting indexing: {:?}", path);
tracing::warn!("File too large: {}", path);
```

## 10. 依赖管理

### 10.1 Cargo 特性

项目使用 Cargo 特性来控制可选依赖：

```toml
[features]
default = ["model-embedding"]
model-embedding = ["candle-core", "candle-nn", "candle-transformers"]
```

- **默认特性**：包含模型嵌入支持
- **可选特性**：可以禁用模型支持，使用纯 TF-IDF

### 10.2 构建命令

```bash
# 完整构建（包含模型支持）
cargo build --release

# 不包含模型支持
cargo build --release --no-default-features
```

## 11. 总结

本项目选择了最适合的技术栈：

1. **Rust + Tokio**：高性能、安全的异步运行时
2. **Tree-sitter**：准确的代码解析
3. **Candle**：本地模型推理，无需 Python
4. **Qdrant**：高效的向量搜索
5. **Axum**：现代化的 Web 框架

这些技术的组合提供了一个高性能、可扩展、易于维护的代码索引和搜索系统。

