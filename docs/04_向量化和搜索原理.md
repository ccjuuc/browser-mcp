# 向量化和搜索原理

本文档详细解释代码向量化的原理、实现方式，以及语义搜索的工作机制。

## 1. 向量化基础概念

### 1.1 什么是向量化？

向量化（Embedding）是将文本、代码等离散数据转换为连续数值向量的过程。这些向量可以在高维空间中表示数据，使得语义相似的内容在向量空间中距离更近。

### 1.2 为什么需要向量化？

- **语义理解**：向量可以捕捉代码的语义信息，而不仅仅是关键词
- **相似度计算**：可以通过向量间的距离（如余弦相似度）计算相似性
- **高效搜索**：向量数据库可以快速进行相似度搜索

### 1.3 向量化示例

```
代码："function add(a, b) { return a + b; }"

向量化后：[0.123, -0.456, 0.789, ..., 0.234]  (768 维)
```

## 2. CodeBERT 模型向量化

### 2.1 CodeBERT 简介

CodeBERT 是微软开发的代码理解模型：

- **基于 BERT**：使用 BERT 架构，但针对代码进行了预训练
- **双向编码**：同时考虑上下文信息
- **多语言支持**：支持多种编程语言
- **输出维度**：768 维向量

### 2.2 模型架构

```
输入文本
  │
  ├─> Tokenizer (分词)
  │     │
  │     └─> Token IDs: [101, 7592, 2088, ...]
  │
  ├─> Embedding Layer (词嵌入)
  │     │
  │     └─> Token Embeddings
  │
  ├─> Transformer Layers (12 层)
  │     │
  │     ├─> Multi-Head Attention
  │     ├─> Feed Forward
  │     └─> Layer Normalization
  │
  └─> [CLS] Token Embedding (768 维)
        │
        └─> 归一化
              │
              └─> 最终向量
```

### 2.3 实现流程详解

#### 步骤 1：分词（Tokenization）

```rust
// 使用 Hugging Face tokenizer
let encoding = tokenizer.encode(text, true)?;
let input_ids: Vec<u32> = encoding
    .get_ids()
    .iter()
    .map(|&id| id as u32)
    .collect();

// 示例：
// 输入: "function add(a, b)"
// 输出: [101, 7592, 2088, 112, 109, 102, 102]
//       [CLS, function, add, (, a, ,, b, )]
```

**关键点**：
- `[CLS]` token：特殊标记，用于表示整个序列的语义
- 子词切分：代码标识符可能被切分成多个 token（如 `function_definition` → `function` + `_` + `definition`）

#### 步骤 2：填充和截断

```rust
// 填充到最大长度
let mut padded_ids = vec![0u32; max_length];  // 0 是 [PAD] token
let len = input_ids.len().min(max_length);
padded_ids[..len].copy_from_slice(&input_ids[..len]);

// 创建 attention mask（标记哪些位置是真实 token）
let attention_mask: Vec<u32> = (0..len)
    .map(|_| 1u32)  // 真实 token
    .chain((len..max_length).map(|_| 0u32))  // 填充位置
    .collect();
```

**为什么需要填充？**：
- 模型需要固定长度的输入
- 不同代码块的 token 数量不同
- 填充到统一长度便于批量处理

#### 步骤 3：模型前向传播

```rust
// 创建输入张量
let input_ids_tensor = Tensor::new(padded_ids, &device)?.unsqueeze(0)?;  // [1, seq_len]
let attention_mask_tensor = Tensor::new(attention_mask, &device)?.unsqueeze(0)?;
let token_type_ids = Tensor::zeros((1, max_length), DType::U32, &device)?;  // 全 0

// BERT 前向传播
let hidden_states = model.forward(
    &input_ids_tensor,
    &token_type_ids,
    Some(&attention_mask_tensor)
)?;

// hidden_states shape: [batch_size, seq_len, hidden_size] = [1, 512, 768]
```

**Transformer 层的处理**：

1. **Self-Attention**：每个 token 关注序列中所有其他 token
   ```
   Attention(Q, K, V) = softmax(QK^T / √d_k) V
   ```
   这使得模型能够理解代码的上下文关系。

2. **Feed Forward**：两层全连接网络
   ```
   FFN(x) = max(0, xW1 + b1)W2 + b2
   ```

3. **残差连接和层归一化**：帮助训练深层网络

#### 步骤 4：提取 [CLS] Token 嵌入

```rust
// [CLS] token 是第一个 token（索引 0）
let cls_embedding = hidden_states
    .get(0)      // batch 的第一个样本
    .and_then(|batch| batch.get(0))  // 序列的第一个 token ([CLS])
    .context("Failed to extract [CLS] token embedding")?;

// cls_embedding shape: [768]
```

**为什么使用 [CLS] token？**：
- BERT 设计时，[CLS] token 被训练来表示整个序列的语义
- 对于分类和检索任务，[CLS] token 包含了最丰富的信息
- 简单高效，不需要额外的池化操作

#### 步骤 5：归一化

```rust
let embedding_vec: Vec<f32> = cls_embedding.to_vec1::<f32>()?;

// L2 归一化
let norm: f32 = embedding_vec.iter().map(|x| x * x).sum::<f32>().sqrt();
let normalized: Vec<f32> = if norm > 0.0 {
    embedding_vec.iter().map(|x| x / norm).collect()
} else {
    embedding_vec
};
```

**归一化的作用**：
- 将向量标准化为单位长度（长度为 1）
- 使得余弦相似度计算更稳定
- 公式：`v_normalized = v / ||v||`

### 2.4 GPU 加速

```rust
// 自动检测并使用 GPU
let device = match Device::cuda_if_available(0) {
    Ok(dev) => {
        tracing::info!("Using CUDA device 0");
        dev
    }
    Err(_) => {
        tracing::warn!("CUDA not available, using CPU");
        Device::Cpu
    }
};
```

**GPU vs CPU**：
- **GPU**：适合并行计算，推理速度快 10-100 倍
- **CPU**：通用计算，速度较慢但兼容性好
- **自动降级**：如果 GPU 不可用，自动使用 CPU

### 2.5 模型文件结构

```
model/
├── config.json          # 模型配置（层数、维度等）
├── model.safetensors    # 模型权重（SafeTensors 格式）
├── tokenizer.json       # 分词器配置
├── vocab.json           # 词汇表（旧格式，可选）
└── merges.txt           # BPE 合并规则（旧格式，可选）
```

## 3. TF-IDF 向量化（降级方案）

### 3.1 TF-IDF 简介

TF-IDF（Term Frequency-Inverse Document Frequency）是一种简单的文本表示方法：

- **TF（词频）**：词在文档中出现的频率
- **IDF（逆文档频率）**：词的稀有程度
- **TF-IDF**：TF × IDF，突出重要且稀有的词

### 3.2 实现原理

```rust
async fn embed_tfidf(&self, text: &str) -> Result<Vec<f32>> {
    // 1. 简单分词
    let tokens = Self::tokenize(text);
    // 示例: "function add" → ["function", "add"]
    
    // 2. 使用哈希映射到向量维度
    let mut vector = vec![0.0; self.config.dimension];  // 768 维
    for token in tokens {
        let hash = Self::hash_token(&token);
        let index = hash % self.config.dimension;
        vector[index] += 1.0;  // 简单计数（TF）
    }
    
    // 3. L2 归一化
    let norm: f32 = vector.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 0.0 {
        for v in &mut vector {
            *v /= norm;
        }
    }
    
    Ok(vector)
}
```

### 3.3 哈希函数

```rust
fn hash_token(token: &str) -> usize {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};
    
    let mut hasher = DefaultHasher::new();
    token.hash(&mut hasher);
    hasher.finish() as usize
}
```

**哈希向量化的优缺点**：

**优点**：
- ✅ 不需要模型文件
- ✅ 计算速度快
- ✅ 内存占用小
- ✅ 易于实现

**缺点**：
- ❌ 无法理解语义
- ❌ 哈希冲突可能影响准确性
- ❌ 只是词频统计，丢失语法信息

### 3.4 分词实现

```rust
fn tokenize(text: &str) -> Vec<String> {
    text.to_lowercase()
        .chars()
        .map(|c| if c.is_alphanumeric() || c.is_whitespace() {
            c
        } else {
            ' '  // 将标点符号替换为空格
        })
        .collect::<String>()
        .split_whitespace()
        .filter(|s| s.len() > 2)  // 过滤太短的词
        .map(|s| s.to_string())
        .collect()
}
```

## 4. 语义搜索原理

### 4.1 余弦相似度

语义搜索的核心是计算向量间的相似度，使用余弦相似度：

```
cosine_similarity(a, b) = (a · b) / (||a|| × ||b||)
```

**几何意义**：
- 计算两个向量夹角的余弦值
- 值域：[-1, 1]
- 1 表示完全相同，0 表示正交（无关），-1 表示完全相反

**为什么使用余弦相似度？**：
- 只关注向量的方向，忽略长度
- 对于归一化向量，等价于点积
- 计算简单高效

### 4.2 实现代码

```rust
pub fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    if a.len() != b.len() {
        return 0.0;
    }
    
    // 点积
    let dot_product: f32 = a.iter().zip(b.iter())
        .map(|(x, y)| x * y)
        .sum();
    
    // L2 范数
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    
    if norm_a == 0.0 || norm_b == 0.0 {
        return 0.0;
    }
    
    dot_product / (norm_a * norm_b)
}
```

### 4.3 搜索流程

```
1. 用户查询: "如何实现视频解码？"
   │
   └─> Embedder.embed(query)
         │
         └─> 查询向量: [0.1, -0.2, 0.3, ..., 0.4]
               │
               └─> QdrantStorage.search(query_vector, limit=10)
                     │
                     ├─> 对每个存储的向量计算余弦相似度
                     │     │
                     │     └─> similarity = cosine(query_vec, stored_vec)
                     │
                     ├─> 按相似度排序
                     │
                     └─> 返回 top-10 最相似的代码块
                           │
                           └─> [(chunk1, 0.95), (chunk2, 0.89), ...]
```

### 4.4 Qdrant 向量搜索

Qdrant 使用高效的索引结构（如 HNSW）加速搜索：

```rust
pub async fn search(&self, vector: Vec<f32>, limit: u64) -> Result<Vec<(CodeChunk, f32)>> {
    let search_result = self.client.search_points(SearchPoints {
        collection_name: self.collection_name.clone(),
        vector,           // 查询向量
        limit,            // 返回数量
        with_payload: Some(true.into()),  // 返回元数据
    }).await?;
    
    // Qdrant 内部：
    // 1. 使用 HNSW 索引快速找到近似最近邻
    // 2. 计算精确的余弦相似度
    // 3. 排序并返回 top-k
}
```

**HNSW 索引**：
- Hierarchical Navigable Small World
- 多层次的图结构
- 搜索时间复杂度：O(log N)，N 是向量数量
- 比线性扫描快数千倍

## 5. 向量化的代码块构建

### 5.1 代码块文本构建

在向量化之前，需要构建用于向量化的文本：

```rust
// 构建向量化文本
let mut text = String::new();
if let Some(ref name) = chunk.node_name {
    text.push_str(&format!("{} ", name));  // 节点名称（如函数名）
}
text.push_str(&chunk.node_type);          // 节点类型（如 "function_item"）
text.push_str(" ");
text.push_str(&chunk.content);            // 代码内容

// 示例：
// "add function_item function add(a, b) { return a + b; }"
```

**为什么包含这些信息？**：
- **节点名称**：函数名、类名等包含重要的语义信息
- **节点类型**：帮助模型理解代码结构
- **代码内容**：完整的代码语义

### 5.2 批量向量化

为了提高效率，支持批量向量化：

```rust
pub async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
    if let (Some(ref model), Some(ref tokenizer)) = (&self.model, &self.tokenizer) {
        // 批量处理（虽然当前是逐个处理，但可以优化为真正的批量）
        let mut embeddings = Vec::new();
        for text in texts {
            embeddings.push(self.embed_with_model(text, Arc::clone(model), tokenizer).await?);
        }
        Ok(embeddings)
    } else {
        // TF-IDF 批量处理
        // ...
    }
}
```

**未来优化**：可以实现真正的批量推理，一次处理多个文本，提高 GPU 利用率。

## 6. CodeBERT vs TF-IDF 对比

| 特性 | CodeBERT | TF-IDF |
|------|----------|--------|
| **语义理解** | ✅ 强 | ❌ 弱（只是词频） |
| **准确度** | ✅ 高 | ⚠️ 中等 |
| **速度** | ⚠️ 较慢（需要模型推理） | ✅ 快 |
| **资源占用** | ❌ 大（需要 GPU/CPU 和内存） | ✅ 小 |
| **依赖** | ✅ 需要模型文件 | ✅ 无需依赖 |
| **适用场景** | 语义搜索、代码理解 | 快速搜索、资源受限环境 |

## 7. 实际应用示例

### 7.1 索引阶段

```rust
// 1. 解析代码文件
let chunks = parser.parse_file("src/main.rs", content, 2000)?;
// 得到: [CodeChunk { node_name: "main", content: "...", ... }, ...]

// 2. 为每个代码块生成向量
for chunk in &chunks {
    let text = format!("{} {} {}", chunk.node_name, chunk.node_type, chunk.content);
    let embedding = embedder.embed(&text).await?;
    chunk.embedding = Some(embedding);
}

// 3. 存储到 Qdrant
qdrant.upsert_chunks(chunks).await?;
```

### 7.2 搜索阶段

```rust
// 1. 用户查询
let query = "如何实现视频解码？";

// 2. 将查询转换为向量
let query_vector = embedder.embed(query).await?;

// 3. 在向量数据库中搜索
let results = qdrant.search(query_vector, 10).await?;

// 4. 返回结果
for (chunk, similarity) in results {
    println!("相似度: {:.2}%", similarity * 100.0);
    println!("文件: {}", chunk.file_path);
    println!("代码: {}", chunk.content);
}
```

## 8. 性能优化建议

### 8.1 模型推理优化

- **批量处理**：一次处理多个文本，提高 GPU 利用率
- **量化**：使用 INT8 量化减少内存和加速
- **模型剪枝**：移除不重要的权重

### 8.2 向量搜索优化

- **索引优化**：调整 Qdrant 的 HNSW 参数
- **过滤**：使用 payload 过滤，减少搜索空间
- **缓存**：缓存常用查询的结果

## 9. 总结

向量化是语义搜索的核心技术：

1. **CodeBERT**：提供强大的语义理解能力，适合高质量搜索
2. **TF-IDF**：简单快速的降级方案，适合资源受限环境
3. **余弦相似度**：计算向量相似性的标准方法
4. **向量数据库**：高效的相似度搜索基础设施

通过向量化，我们可以让 AI 助手理解代码的语义，而不仅仅是匹配关键词，从而实现更智能的代码搜索和理解。

